{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 31,
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import ml_metrics as metrics"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "source": [
    "df = pd.read_csv(\"../data/COLING/skill_prediction - prediction.csv\")\n",
    "df.head(1)"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "                   Job description /job requirements  \\\n",
       "0  [CLS] requirements technical academic educatio...   \n",
       "\n",
       "                                  COLING predictions  \\\n",
       "0  ['Customer Service', 'Leadership', 'Microsoft ...   \n",
       "\n",
       "                                              GALAXC  \\\n",
       "0  ['Product Development', 'Manufacturing', 'Nego...   \n",
       "\n",
       "                                         True Labels  \\\n",
       "0  ['Management', 'Microsoft Office', 'Cloud Comp...   \n",
       "\n",
       "                                       GALAXC-latest  \n",
       "0  ['Product Development', 'Manufacturing', 'Cros...  "
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Job description /job requirements</th>\n",
       "      <th>COLING predictions</th>\n",
       "      <th>GALAXC</th>\n",
       "      <th>True Labels</th>\n",
       "      <th>GALAXC-latest</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[CLS] requirements technical academic educatio...</td>\n",
       "      <td>['Customer Service', 'Leadership', 'Microsoft ...</td>\n",
       "      <td>['Product Development', 'Manufacturing', 'Nego...</td>\n",
       "      <td>['Management', 'Microsoft Office', 'Cloud Comp...</td>\n",
       "      <td>['Product Development', 'Manufacturing', 'Cros...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "metadata": {},
     "execution_count": 6
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "source": [
    "true= df['True Labels'].apply( lambda x: eval(x))\n",
    "true = true.tolist()\n",
    "# coling = df['COLING predictions'].apply( lambda x: eval(x))\n",
    "\n",
    "# coling = coling.tolist()\n",
    "Ga = df['GALAXC'].apply( lambda x: eval(x))\n",
    "GAL = df['GALAXC-latest'].apply( lambda x: eval(x))\n",
    "Ga = Ga.tolist()\n",
    "GAL = GAL.tolist()\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "source": [],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "source": [
    "mapk(true,GAL,40)"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "0.2107385527500831"
      ]
     },
     "metadata": {},
     "execution_count": 52
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "source": [
    "for item in GAL:\n",
    "    assert len(item) ==20"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "class Metric:\n",
    "    def __init__(self):\n",
    "        self.epsilon = 1.0e-4\n",
    "        pass\n",
    "\n",
    "    def __call__(self, outputs, target):\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def reset(self):\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def value(self):\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def name(self):\n",
    "        raise NotImplementedError\n",
    "\n",
    "class Accuracy(Metric):\n",
    "    def __init__(self,topK):\n",
    "        super(Accuracy,self).__init__()\n",
    "        self.topK = topK\n",
    "        self.reset()\n",
    "\n",
    "    def __call__(self, logits, target):\n",
    "        _, pred = logits.topk(self.topK, 1, True, True)\n",
    "        pred = pred.t()\n",
    "        correct = pred.eq(target.view(1, -1).expand_as(pred))\n",
    "        self.correct_k = correct[:self.topK].view(-1).float().sum(0)\n",
    "        self.total = target.size(0)\n",
    "\n",
    "    def reset(self):\n",
    "        self.correct_k = 0\n",
    "        self.total = 0\n",
    "\n",
    "    def value(self):\n",
    "        return float(self.correct_k)  / self.total\n",
    "\n",
    "    def name(self):\n",
    "        return 'accuracy'\n",
    "\n",
    "\n",
    "class AccuracyThresh(Metric):\n",
    "    def __init__(self,thresh = 0.5):\n",
    "        super(AccuracyThresh,self).__init__()\n",
    "        self.thresh = thresh\n",
    "        self.reset()\n",
    "\n",
    "    def __call__(self, logits, target):\n",
    "        self.y_pred = logits.sigmoid()\n",
    "        self.y_true = target\n",
    "\n",
    "    def reset(self):\n",
    "        self.correct_k = 0\n",
    "        self.total = 0\n",
    "\n",
    "    def value(self):\n",
    "        data_size = self.y_pred.size(0)\n",
    "        acc = np.mean(((self.y_pred>self.thresh).type(torch.DoubleTensor)==self.y_true.type(torch.DoubleTensor)).float().cpu().numpy(), axis=1).sum()\n",
    "        return acc / data_size\n",
    "\n",
    "    def name(self):\n",
    "        return 'accuracy'\n",
    "\n",
    "class MRR(Metric):\n",
    "    def __init__(self):\n",
    "        super(MRR,self).__init__()\n",
    "        self.rr = []\n",
    "\n",
    "    def __call__(self, logits, target):\n",
    "        self.y_pred = logits.sigmoid().cpu().numpy()\n",
    "        self.y_true = target.cpu().numpy()\n",
    "\n",
    "        for i in range(len(self.y_pred)):\n",
    "            y_t = np.where(self.y_true[i]==1)[0]\n",
    "            y_t_len = len(y_t)\n",
    "            y_p_index = np.flip(np.argsort(self.y_pred[i]),0)\n",
    "            for i in range(len(y_p_index)):\n",
    "                if y_p_index[i] in y_t:\n",
    "                    self.rr.append(1/float(i+1))\n",
    "                    break\n",
    "\n",
    "    def reset(self):\n",
    "        return\n",
    "\n",
    "    def value(self):\n",
    "        return np.mean(np.array(self.rr))*100\n",
    "\n",
    "    def name(self):\n",
    "        return 'mrr'\n",
    "\n",
    "    def show(self):\n",
    "        print(\"MRR: {}\".format(self.value()))\n",
    "\n",
    "class NDCG(Metric):\n",
    "    def __init__(self):\n",
    "        super(NDCG,self).__init__()\n",
    "        self.NDCG = []\n",
    "        self.kvals = [5,10,30,50,100]\n",
    "        self.reset()\n",
    "\n",
    "    def __call__(self, logits, target):\n",
    "        self.y_pred = logits.sigmoid().cpu().numpy()\n",
    "        self.y_true = target.cpu().numpy()\n",
    "        \n",
    "        for i in range(len(self.y_pred)):\n",
    "            y_t = np.where(self.y_true[i]==1)[0]\n",
    "            y_t_len = len(y_t)\n",
    "            y_p_index = np.flip(np.argsort(self.y_pred[i]),0)\n",
    "            idcg = np.sum([1.0/np.log2(x+2) for x in range(y_t_len)])\n",
    "            ndcg = []\n",
    "            for k in self.kvals:\n",
    "                dcg = 0 \n",
    "                for i in range(0,k):\n",
    "                    if y_p_index[i] in y_t:\n",
    "                        dcg = dcg + 1.0/np.log2(i+2)\n",
    "                ndcg.append(dcg/idcg)\n",
    "            self.NDCG.append(ndcg)\n",
    "\n",
    "    def reset(self):\n",
    "        return\n",
    "\n",
    "    def value(self):\n",
    "        return np.mean(np.array(self.NDCG),axis=0)*100\n",
    "\n",
    "    def name(self):\n",
    "        return 'ndcg'\n",
    "\n",
    "    def show(self):\n",
    "        idx = 0\n",
    "        result = self.value()\n",
    "        for k in self.kvals:\n",
    "            print(\"NDCG@{}: {}\".format(k,result[idx]))\n",
    "            idx += 1\n",
    "        return result\n",
    "\n",
    "class Recall(Metric):\n",
    "    def __init__(self):\n",
    "        super(Recall,self).__init__()\n",
    "        self.Recall = []\n",
    "        self.kvals = [5,10,20,30,50,100]\n",
    "        self.reset()\n",
    "\n",
    "    def __call__(self, logits, target):\n",
    "        self.y_pred = logits.sigmoid().cpu().numpy()        \n",
    "        self.y_true = target.cpu().numpy()\n",
    "        for i in range(len(self.y_pred)):\n",
    "            recall = []\n",
    "            y_p_index = np.flip(np.argsort(self.y_pred[i]),0) # What is this doing, needs to sort highest probability\n",
    "            y_t = np.where(self.y_true[i]==1)[0] # Returns index where condition true. an array \n",
    "            y_t_len = len(y_t)\n",
    "\n",
    "            for k in self.kvals:\n",
    "                correct = len(np.intersect1d(y_t,y_p_index[0:k]))\n",
    "                recall.append(correct/(y_t_len+self.epsilon))\n",
    "            self.Recall.append(recall)\n",
    "        \n",
    "\n",
    "    def reset(self):\n",
    "        return\n",
    "\n",
    "    def value(self):\n",
    "        return np.mean(np.array(self.Recall),axis=0)*100\n",
    "\n",
    "    def name(self):\n",
    "        return 'recall'\n",
    "    \n",
    "    def show(self):\n",
    "        idx = 0\n",
    "        result = self.value()\n",
    "        for k in self.kvals:\n",
    "            print(\"Recall@{}: {}\".format(k,result[idx]))\n",
    "            idx += 1\n",
    "        return result\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class Precision(Metric):\n",
    "    def __init__(self):\n",
    "        super(Precision,self).__init__()\n",
    "        self.Precision = []\n",
    "        self.kvals = [5,10,20,30,50,100]\n",
    "        self.reset()\n",
    "        self.count_ind=[]\n",
    "\n",
    "    def __call__(self, logits, target):\n",
    "        self.y_pred = logits.sigmoid().cpu().numpy()       \n",
    "        self.y_true = target.cpu().numpy()\n",
    "        for i in range(len(self.y_pred)):\n",
    "            precision = []\n",
    "            y_p_index = np.flip(np.argsort(self.y_pred[i]),0) # index where predicted values are true\n",
    "            y_t = np.where(self.y_true[i]==1)[0] # indexes where actual truth labels \n",
    "            y_t_len = len(y_t)  # how many true labels\n",
    "\n",
    "            # print(\"Number of skills actually correct :\",y_t_len)\n",
    "            # print(\"Number of skills we predict are correct for k 5,10,30,50,100:\")\n",
    "\n",
    "            for k in self.kvals:\n",
    "                correct = len(np.intersect1d(y_t,y_p_index[0:k]))\n",
    "                # print(correct)\n",
    "                precision.append(correct/(k+self.epsilon))\n",
    "            self.Precision.append(precision)\n",
    "            # print(\"Indexes matched\",np.intersect1d(y_t,y_p_index[0:k]))\n",
    "        \n",
    "\n",
    "    def reset(self):\n",
    "        return\n",
    "\n",
    "    def value(self):\n",
    "        return np.mean(np.array(self.Precision),axis=0)*100\n",
    "\n",
    "    def name(self):\n",
    "        return 'precision'\n",
    "    \n",
    "    def show(self):\n",
    "        idx = 0\n",
    "        result = self.value()\n",
    "        for k in self.kvals:\n",
    "            print(\"Precision@{}: {}\".format(k,result[idx]))\n",
    "            idx += 1\n",
    "        return result\n",
    "# implicit metrics are evaluated based on the top-20 predections \n",
    "\n",
    "class Implicit_Metrics(Metric):\n",
    "    skill_list = []\n",
    "    i2w = {}\n",
    "    i2l = {}\n",
    "\n",
    "    def __init__(self, skill_list_path, i2w, i2l):\n",
    "        super(Implicit_Metrics,self).__init__()\n",
    "        if not self.skill_list:\n",
    "            self.skill_list = self.read_skills(path=skill_list_path)\n",
    "        if not self.i2w:\n",
    "            self.i2w = i2w\n",
    "        if not self.i2l:\n",
    "            self.i2l = i2l\n",
    "    \n",
    "    def read_skills(self, path):\n",
    "        skill_list = []\n",
    "        with open(path, 'r') as f:\n",
    "            for line in f.readlines():\n",
    "                skill = line.replace('\\n','')\n",
    "                skill_list.append(skill)\n",
    "        return skill_list\n",
    "\n",
    "    def present_skills(self, job_description, skill_list):\n",
    "        skills = []\n",
    "        for skill in skill_list:\n",
    "            if (' ' + skill.lower().strip() + ' ') in (' ' + job_description.lower().strip() + ' '):\n",
    "                if skill not in skills:\n",
    "                    skills.append(skill)\n",
    "        return skills\n",
    "\n",
    "    def generate_sentence(self, input_ids):\n",
    "        input_ids = input_ids.tolist()\n",
    "        sentence = \"\"\n",
    "        for idx in input_ids:\n",
    "            if self.i2w[idx][:2]==\"##\":\n",
    "                sentence += self.i2w[idx][2:]\n",
    "            else:\n",
    "                sentence += \" \" + self.i2w[idx]\n",
    "        return sentence.strip()\n",
    "    \n",
    "    def job_labels(self, label_indices):\n",
    "        labels = []\n",
    "        for idx in label_indices:\n",
    "            labels.append(self.i2l[idx])\n",
    "        return labels\n",
    "\n",
    "    def intersection(self, lst1, lst2):\n",
    "        lst3 = [value for value in lst1 if value in lst2]\n",
    "        return lst3\n",
    "\n",
    "class EIM(Implicit_Metrics):\n",
    "    def __init__(self, path, i2w, i2l):\n",
    "        super(EIM,self).__init__(path, i2w, i2l)\n",
    "        self.EIM = []\n",
    "        self.reset()\n",
    "\n",
    "    def __call__(self, input_ids, output, target):\n",
    "        self.input_ids = input_ids\n",
    "        self.y_pred = output.sigmoid().cpu().numpy()        \n",
    "        self.y_true = target.cpu().numpy()\n",
    "        for i in range(len(self.y_pred)):\n",
    "            self.EIM.append(self.eim(self.input_ids[i], self.y_pred[i], self.y_true[i]))\n",
    "        \n",
    "            \n",
    "    def eim(self, input_ids, output, target):\n",
    "        job_description = self.generate_sentence(input_ids)    \n",
    "        sorted_prediction_indices = np.flip(np.argsort(output))\n",
    "        label_indices = np.where(target==1)[0]\n",
    "        sorted_prediction_indices = sorted_prediction_indices[:len(label_indices)]\n",
    "        \n",
    "        \n",
    "        job_labels = self.job_labels(label_indices) # skill in given req skills\n",
    "        predicted_labels = self.job_labels(sorted_prediction_indices) # skill for jd prediction\n",
    "        explicit_skills = self.present_skills(job_description, self.skill_list) # skill in jd against all skills \n",
    "        \n",
    "        ### CODE ADDED TO SEE WHICH SKILLS ARE MISSED AND WHICH ARE ADDITIONALLY ADDED ####\n",
    "        ### Job req, Req skills, prediction\n",
    "        ### 0,0,1\n",
    "        ### 0,1,0\n",
    "        ### 0, 1, 1\n",
    "        ### 1, 0, 0 and so on. storing in a list. and taking bitwise\n",
    "\n",
    "        final_store =[[] for i in range(7)]\n",
    "        import itertools\n",
    "\n",
    "        lst = list(map(list, itertools.product([0, 1], repeat=3))) # generate bitwise list of batches of 3\n",
    "        lst = lst[1:]\n",
    "        for skill in self.skill_list:\n",
    "            for i,l in enumerate(lst):\n",
    "                count =0\n",
    "                for bit,val in zip(l,[explicit_skills,job_labels,predicted_labels]):\n",
    "                    count += 1 - (bit ^ (skill in val))\n",
    "                if count == 3:\n",
    "                    final_store[i].append(skill)\n",
    "        with open('skills_magnified.txt','a+') as f:\n",
    "            for values in final_store[:-1]:\n",
    "                f.write(str(values)+'\\t')\n",
    "            f.write(str(final_store[-1])+'\\n')\n",
    "        with open(\"predictions.txt\",'a+') as f:\n",
    "            f.write(str(predicted_labels))\n",
    "            f.write('\\n')\n",
    "\n",
    "        ########### Changed ############### \n",
    "        # for item in explicit_skills:\n",
    "        #     if item not in predicted_labels and item not in job_labels:\n",
    "        #         extr_skills.append(item)\n",
    "        #     elif item not in predicted_labels and item in job_labels:\n",
    "        #         missed_skills.append(item)\n",
    "        #     elif item in predicted_labels and item not in  job_labels:\n",
    "        #         new_skills.append(item)\n",
    "\n",
    "            \n",
    "        # with open('skills_magnified.txt','a') as f:\n",
    "        #     f.write(job_description+'\\t'+str(job_labels)+'\\t'+str(extr_skills)+'\\t'+str(missed_skills)+'\\t'+str(new_skills))\n",
    "        #     f.write('\\n')\n",
    "            \n",
    "        ##########  END PREV CODE ################\n",
    "\n",
    "        if len(self.intersection(job_labels, explicit_skills))==0:\n",
    "            return 1\n",
    "        return len(self.intersection(predicted_labels, explicit_skills)) / (len(self.intersection(job_labels, explicit_skills)))\n",
    "\n",
    "\n",
    "    def reset(self):\n",
    "        return\n",
    "\n",
    "    def value(self):\n",
    "        return np.mean(np.array(self.EIM))*100\n",
    "\n",
    "    def name(self):\n",
    "        return 'eim'\n",
    "    \n",
    "    def show(self):\n",
    "        result = self.value()\n",
    "        print(\"EIM: {}\".format(result))\n",
    "\n",
    "class RIIM(Implicit_Metrics):\n",
    "    def __init__(self, path, i2w, i2l):\n",
    "        super(RIIM,self).__init__(path, i2w, i2l)\n",
    "        self.RIIM = []\n",
    "        self.reset()\n",
    "\n",
    "    def __call__(self, input_ids, output, target):\n",
    "        self.input_ids = input_ids\n",
    "        self.y_pred = output.sigmoid().cpu().numpy()        \n",
    "        self.y_true = target.cpu().numpy()\n",
    "        for i in range(len(self.y_pred)):\n",
    "            self.RIIM.append(self.riim(self.input_ids[i], self.y_pred[i], self.y_true[i]))\n",
    "        \n",
    "    def riim(self, input_ids, output, target):\n",
    "        job_description = self.generate_sentence(input_ids)\n",
    "        sorted_prediction_indices = np.flip(np.argsort(output))\n",
    "        label_indices = np.where(target==1)[0]\n",
    "        sorted_prediction_indices = sorted_prediction_indices[:len(label_indices)]\n",
    "        \n",
    "        \n",
    "        job_labels = self.job_labels(label_indices)\n",
    "        predicted_labels = self.job_labels(sorted_prediction_indices)\n",
    "        explicit_skills = self.present_skills(job_description, self.skill_list)\n",
    "        implicit_skills = [val for val in job_labels if val not in explicit_skills]\n",
    "        predicted_implicit_skills = [val for val in predicted_labels if val in implicit_skills]\n",
    "\n",
    "        return len(predicted_implicit_skills)/(len(implicit_skills) + self.epsilon)\n",
    "\n",
    "    def reset(self):\n",
    "        return\n",
    "\n",
    "    def value(self):\n",
    "        return np.mean(np.array(self.RIIM))*100\n",
    "\n",
    "    def name(self):\n",
    "        return 'riim'\n",
    "    \n",
    "    def show(self):\n",
    "        result = self.value()\n",
    "        print(\"RIIM: {}\".format(result))\n",
    "\n",
    "class REIM(Implicit_Metrics):\n",
    "    def __init__(self, path, i2w, i2l):\n",
    "        super(REIM,self).__init__(path, i2w, i2l)\n",
    "        self.REIM = []\n",
    "        self.reset()\n",
    "\n",
    "    def __call__(self, input_ids, output, target):\n",
    "        self.input_ids = input_ids\n",
    "        self.y_pred = output.sigmoid().cpu().numpy()        \n",
    "        self.y_true = target.cpu().numpy()\n",
    "        for i in range(len(self.y_pred)):\n",
    "            self.REIM.append(self.reim(self.input_ids[i], self.y_pred[i], self.y_true[i]))\n",
    "        \n",
    "            \n",
    "    def reim(self, input_ids, output, target):\n",
    "        job_description = self.generate_sentence(input_ids)\n",
    "        sorted_prediction_indices = np.flip(np.argsort(output))\n",
    "        label_indices = np.where(target==1)[0]\n",
    "        sorted_prediction_indices = sorted_prediction_indices[:len(label_indices)]\n",
    "        \n",
    "        \n",
    "        job_labels = self.job_labels(label_indices)\n",
    "        predicted_labels = self.job_labels(sorted_prediction_indices)\n",
    "        explicit_skills = self.present_skills(job_description, self.skill_list)\n",
    "\n",
    "        return len(self.intersection(predicted_labels, explicit_skills)) / (len(explicit_skills) + self.epsilon)\n",
    "\n",
    "\n",
    "    def reset(self):\n",
    "        return\n",
    "\n",
    "    def value(self):\n",
    "        return np.mean(np.array(self.REIM))*100\n",
    "\n",
    "    def name(self):\n",
    "        return 'reim'\n",
    "    \n",
    "    def show(self):\n",
    "        result = self.value()\n",
    "        print(\"REIM: {}\".format(result))\n",
    "\n",
    "\n",
    "class AUC(Metric):\n",
    "    '''\n",
    "    AUC score\n",
    "    micro:\n",
    "            Calculate metrics globally by considering each element of the label\n",
    "            indicator matrix as a label.\n",
    "    macro:\n",
    "            Calculate metrics for each label, and find their unweighted\n",
    "            mean.  This does not take label imbalance into account.\n",
    "    weighted:\n",
    "            Calculate metrics for each label, and find their average, weighted\n",
    "            by support (the number of true instances for each label).\n",
    "    samples:\n",
    "            Calculate metrics for each instance, and find their average.\n",
    "    Example:\n",
    "        >>> metric = AUC(**)\n",
    "        >>> for epoch in range(epochs):\n",
    "        >>>     metric.reset()\n",
    "        >>>     for batch in batchs:\n",
    "        >>>         logits = model()\n",
    "        >>>         metric(logits,target)\n",
    "        >>>         print(metric.name(),metric.value())\n",
    "    '''\n",
    "\n",
    "    def __init__(self,task_type = 'binary',average = 'binary'):\n",
    "        super(AUC, self).__init__()\n",
    "\n",
    "        assert task_type in ['binary','multiclass']\n",
    "        assert average in ['binary','micro', 'macro', 'samples', 'weighted']\n",
    "\n",
    "        self.task_type = task_type\n",
    "        self.average = average\n",
    "\n",
    "    def __call__(self,logits,target):\n",
    "        if self.task_type == 'binary':\n",
    "            self.y_prob = logits.sigmoid().data.cpu().numpy()\n",
    "        else:\n",
    "            self.y_prob = logits.softmax(-1).data.cpu().detach().numpy()\n",
    "        self.y_true = target.cpu().numpy()\n",
    "\n",
    "    def reset(self):\n",
    "        self.y_prob = 0\n",
    "        self.y_true = 0\n",
    "\n",
    "    def value(self):\n",
    "        auc = roc_auc_score(y_score=self.y_prob, y_true=self.y_true, average=self.average)\n",
    "        return auc\n",
    "\n",
    "    def name(self):\n",
    "        return 'auc'\n",
    "\n",
    "class F1Score(Metric):\n",
    "    '''\n",
    "    F1 Score\n",
    "    binary:\n",
    "            Only report results for the class specified by ``pos_label``.\n",
    "            This is applicable only if targets (``y_{true,pred}``) are binary.\n",
    "    micro:\n",
    "            Calculate metrics globally by considering each element of the label\n",
    "            indicator matrix as a label.\n",
    "    macro:\n",
    "            Calculate metrics for each label, and find their unweighted\n",
    "            mean.  This does not take label imbalance into account.\n",
    "    weighted:\n",
    "            Calculate metrics for each label, and find their average, weighted\n",
    "            by support (the number of true instances for each label).\n",
    "    samples:\n",
    "            Calculate metrics for each instance, and find their average.\n",
    "    Example:\n",
    "        >>> metric = F1Score(**)\n",
    "        >>> for epoch in range(epochs):\n",
    "        >>>     metric.reset()\n",
    "        >>>     for batch in batchs:\n",
    "        >>>         logits = model()\n",
    "        >>>         metric(logits,target)\n",
    "        >>>         print(metric.name(),metric.value())\n",
    "    '''\n",
    "    def __init__(self,thresh = 0.5, normalizate = True,task_type = 'multiclass',average = 'micro',search_thresh = False):\n",
    "        super(F1Score).__init__()\n",
    "        assert task_type in ['binary','multiclass']\n",
    "        assert average in ['binary','micro', 'macro', 'samples', 'weighted']\n",
    "\n",
    "        self.thresh = thresh\n",
    "        self.task_type = task_type\n",
    "        self.normalizate  = normalizate\n",
    "        self.search_thresh = search_thresh\n",
    "        self.average = average\n",
    "\n",
    "    def thresh_search(self,y_prob):\n",
    "        best_threshold = 0\n",
    "        best_score = 0\n",
    "        for threshold in tqdm([i * 0.01 for i in range(100)], disable=True):\n",
    "            self.y_pred = y_prob > threshold\n",
    "            score = self.value()\n",
    "            if score > best_score:\n",
    "                best_threshold = threshold\n",
    "                best_score = score\n",
    "        return best_threshold,best_score\n",
    "\n",
    "    def __call__(self,logits,target):\n",
    "        self.y_true = target.cpu().numpy()\n",
    "        if self.normalizate and self.task_type == 'binary':\n",
    "            y_prob = logits.sigmoid().data.cpu().numpy()\n",
    "        elif self.normalizate and self.task_type == 'multiclass':\n",
    "            y_prob = logits.softmax(-1).data.cpu().detach().numpy()\n",
    "        else:\n",
    "            y_prob = logits.cpu().detach().numpy()\n",
    "\n",
    "        if self.task_type == 'binary':\n",
    "            if self.thresh and self.search_thresh == False:\n",
    "                self.y_pred = (y_prob > self.thresh ).astype(int)\n",
    "                self.value()\n",
    "            else:\n",
    "                thresh,f1 = self.thresh_search(y_prob = y_prob)\n",
    "                print(f\"Best thresh: {thresh:.4f} - F1 Score: {f1:.4f}\")\n",
    "\n",
    "        if self.task_type == 'multiclass':\n",
    "            self.y_pred = np.argmax(y_prob, 1)\n",
    "\n",
    "    def reset(self):\n",
    "        self.y_pred = 0\n",
    "        self.y_true = 0\n",
    "\n",
    "    def value(self):\n",
    "        f1 = f1_score(y_true=self.y_true, y_pred=self.y_pred, average=self.average)\n",
    "        return f1\n",
    "\n",
    "    def name(self):\n",
    "        return 'f1'\n",
    "    \n",
    "    def show(self):\n",
    "        result = self.value()\n",
    "        print(\"F1: {}\".format(result))\n",
    "      \n",
    "        \n",
    "\n",
    "class ClassReport(Metric):\n",
    "    '''\n",
    "    class report\n",
    "    '''\n",
    "    def __init__(self,target_names = None):\n",
    "        super(ClassReport).__init__()\n",
    "        self.target_names = target_names\n",
    "\n",
    "    def reset(self):\n",
    "        self.y_pred = 0\n",
    "        self.y_true = 0\n",
    "\n",
    "    def value(self):\n",
    "        score = classification_report(y_true = self.y_true,\n",
    "                                      y_pred = self.y_pred,\n",
    "                                      target_names=self.target_names)\n",
    "        print(f\"\\n\\n classification report: {score}\")\n",
    "\n",
    "    def __call__(self,logits,target):\n",
    "        _, y_pred = torch.max(logits.data, 1)\n",
    "        self.y_pred = y_pred.cpu().numpy()\n",
    "        self.y_true = target.cpu().numpy()\n",
    "\n",
    "    def name(self):\n",
    "        return \"class_report\"\n",
    "\n",
    "class MultiLabelReport(Metric):\n",
    "    '''\n",
    "    multi label report\n",
    "    '''\n",
    "    def __init__(self,id2label = None):\n",
    "        super(MultiLabelReport).__init__()\n",
    "        self.id2label = id2label\n",
    "\n",
    "    def reset(self):\n",
    "        self.y_prob = 0\n",
    "        self.y_true = 0\n",
    "\n",
    "    def __call__(self,logits,target):\n",
    "\n",
    "        self.y_prob = logits.sigmoid().data.cpu().detach().numpy()\n",
    "        self.y_true = target.cpu().numpy()\n",
    "\n",
    "    def value(self):\n",
    "        for i, label in self.id2label.items():\n",
    "            auc = roc_auc_score(y_score=self.y_prob[:, i], y_true=self.y_true[:, i])\n",
    "            print(f\"label:{label} - auc: {auc:.4f}\")\n",
    "\n",
    "    def name(self):\n",
    "        return \"multilabel_report\"\n"
   ],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3 (ipykernel)",
   "language": "python"
  },
  "language_info": {
   "name": "python",
   "version": "3.6.9",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}